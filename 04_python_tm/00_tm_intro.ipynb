{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-reference",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Learn Python in 10 days (Sams, YouTube video) or Teach Yourself Programming in 10 Years [Peter Norvig's take on the issue](https://norvig.com/21-days.html).\n",
    "\n",
    "Here the angle is pragmatic: Working with (a large amount of) text files what are the possibilities? What do you need to do some useful work? And, when things start to get more serious, how do I get to cooperate fruitfully with programmers?\n",
    "\n",
    "One thing to keep in mind during this workshop is that, when we are looking at files with code, or clever constructs on the command line, etc., we are looking at *end products*, carefully crafted, optimized, made by programmers. Often the clean code we see is not what the maker started out with, but we do not see the start and the intermediate steps (and mistakes) that resulted in the end product.\n",
    "\n",
    "So, the next half hour, we will look at some CLI magic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-promise",
   "metadata": {},
   "source": [
    "### Text mining intro\n",
    "\n",
    "This Jupyter notebook is meant to give you a feel for working with files in the context of textual analysis:\n",
    "\n",
    "    - I have downloaded these files from Web of Science but what is actually there? (use your editor);\n",
    "    - Right, actually I am just interested in a small portion of what I got from WoS (use some text extraction);\n",
    "    - Ok, this is not what I had in mind (go back to the source), or: prepare to do some serious pre-processing.\n",
    "\n",
    "Farfetched examples? Not at all. But one does not have to setup an extended programming environment in order to get this kind of information. Something called the \"Unix uitlities\" (working at the command line) will do nicely.\n",
    "\n",
    "The examples below are taken from Kenneth Ward Church: Unix(tm) for Poets (https://www.cs.upc.edu/~padro/Unixforpoets.pdf)\n",
    "\n",
    "Another good source is: Jeroen Janssens' Data Science at the Command Line (https://www.datascienceatthecommandline.com/1e/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-april",
   "metadata": {},
   "source": [
    "### Jane Austen: Pride and prejudice\n",
    "\n",
    "The Manning book \"Natural Language Processing in Action\" (NLPIA) comes with some example files. These example files are, more often than not, prepared files. In other words, the files are already customized for the software the authors use (for example: Every sentence is on one line (terminated by a newline+) instead of have formatted text that is not wider than 79 characters, each physical line (possibly part of a sentence) ended by a newline+.\n",
    "\n",
    "The NLPIA file is: pride.txt\n",
    "\n",
    "If we open this file in our editor of choice, we \"see\" the formatting. Each sentence in the book is a paragraph, that is there are 2 newlines between sentences (here: sequences of characters terminated by a full stop). There is ONE newline that chops up a book sentence into more or less equal parts to display on the screen.\n",
    "\n",
    "This might not seem a big deal at first glance, but there are all sort of possible glitches right from the start. How many book sentences does P&P have? We can count the double newlines. In my editor that gives 1866 book sentences.\n",
    "\n",
    "Suppose our software can only deal with book sentences on one line in our file, what to do? Easy peasy: Just replace single newlines by a space and double newlines by a single newline. *What could go possibly wrong? Can you spot the caveat?*\n",
    "\n",
    "In the real world out there things are different. If we download the Jane Austen ook from Project Gutenberg, we can choose the format. In our case we choose the plain text UTF-8 file.\n",
    "\n",
    "http://www.gutenberg.org/ebooks/1342\n",
    "\n",
    "If we open this file in our editor, we see something different. The beginning of the first book sentence \"It is a truth universally acknowledged ...\" starts on line 167 in our file. Then the text becomes familiar until the very end where the sequence *** END OF ... signals text added by Project Gutenberg.\n",
    "\n",
    "The NLPIA authors simply removed the Project Gutenberg sandwich around the Jane Austen text, which makes perfect sense if one is going to do textual analysis of that text.\n",
    "\n",
    "#### Conclusions\n",
    "\n",
    "1. It pays off to know the basics of a so-called programming editor:\n",
    "\n",
    "    - Inspect what is there;\n",
    "    - Make some quick necessary changes (re-formatting, deleting the Gutenberg sandwich, etc.) When mistakes are made these editors allow for undo;\n",
    "    - When the file is saved, we save plain text in UTF-8.\n",
    "\n",
    "2. If we were to start with the Project Gutenberg file, we end up with 2 files right at the beginning of our project. We can store the PG file we got from their website as a source file (for example in a directory named \"src\" or \"raw\" and the pre-processed file in another dierctory \"wip\".\n",
    "\n",
    "3. If one really dives in files in different stages tend to pile up. If that is the case simple version control routines (we use Git) can be of tremendous help. If you reached a certain result, you check that particular result in -- as a snapshot. And just as undo comes in handy, these snapshots can be used to roll back to after you messed up things big time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-capitol",
   "metadata": {},
   "source": [
    "Now that we have some idea of the environment we need to do our textmining work, we can explore some basic tasks without diving to deep into prgramming (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-discharge",
   "metadata": {},
   "source": [
    "### The Command Line Interface (CLI)\n",
    "\n",
    "The CLI or terminal a bit Unix/Linux and MacOS oriented; MS Windows users must use a workaround. For the remaining part of this workshop we will concentrate on working with Python programs and code via Jupyter notebooks. But having a glimpse of some of the CLI possibilities not only shows how powerful these tools are, but, more importantly, can show some important concepts of working with textual data:\n",
    "\n",
    "    - chunking running plain text into meaningful chunks;\n",
    "    - discarding parts of the text;\n",
    "    - pipelines, the combination of small code snippets that, in a series of steps, produce the desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-muscle",
   "metadata": {},
   "source": [
    "#### Let's have a quick look at our text\n",
    "\n",
    "First 5 sentences of P&P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sufficient-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sed 5q < data/pride.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-cyprus",
   "metadata": {},
   "source": [
    "#### Count words in a text\n",
    "\n",
    "We break up this task in three discrete steps:\n",
    "\n",
    "1. We break up the text into a sequence of words (or tokens => tokenizing) with tr\n",
    "2. We then sort all the words in our sequence with sort\n",
    "3. Then we count the duplicates with uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-collaboration",
   "metadata": {},
   "source": [
    "Let's try to break up our text in tokens (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "higher-ballot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It\n",
      "is\n",
      "a\n",
      "truth\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-clerk",
   "metadata": {},
   "source": [
    "Fine, works like a charm. And, what is more important, with tr we manipulated newlines like a breeze. Usually newlines are a pain in the proverbial. They can mean the end of a line or a command (depends on the context; but remember in my editor I had to enter the magic combo 'CTRL-Q CTRL-J' twice to select all \"\\n\\n\" sequneces. Here tr let us use the octal representation of the newline char: '\\012*'. Perfect.\n",
    "\n",
    "Let's sort our sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neutral-knight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt |\n",
    "sort | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-constant",
   "metadata": {},
   "source": [
    "Next we count the number of occurences of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "according-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "   1905 a\n",
      "     49 A\n",
      "      1 abatement\n",
      "      6 abhorrence\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt |\n",
    "sort |\n",
    "uniq -c | sed 5q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disciplinary-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4113 to\n",
      "   4058 the\n",
      "   3599 of\n",
      "   3430 and\n",
      "   2138 her\n",
      "   2070 I\n",
      "   1905 a\n",
      "   1843 was\n",
      "   1795 in\n",
      "   1527 that\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt |\n",
    "sort |\n",
    "uniq -c |\n",
    "sort -nr | sed 10q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-scotland",
   "metadata": {},
   "source": [
    "The important thing in the above code snippets are not the somewhat cryptic and terse commands, but the ease of gluing these commands together in a so-called \"pipeline\" using the Unix pipe symbol \"|\" where the output of one command becomes the input of the next. This allows for fast experimenting and adjusting our snippets.\n",
    "\n",
    "The output of the snippet in cell [4] shows that we count 1905 occurences for the lowercase 'a' and 49 for the uppercase 'A'. With our friend tr these are easily merged, we just cast all tokens to lowercase: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "large-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "   1954 a\n",
      "      1 abatement\n",
      "      6 abhorrence\n",
      "      1 abhorrent\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr '[A-Z]' '[a-z]' < data/pride.txt |\n",
    "tr -sc '[a-z]' '[\\012*]' |\n",
    "sort |\n",
    "uniq -c | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-indiana",
   "metadata": {},
   "source": [
    "Right we use the new snippet for our counting pipeline, but this time we do not reverse the order, so we get the least used words first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "possible-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "      1 abatement\n",
      "      1 abhorrent\n",
      "      1 abide\n",
      "      1 abiding\n",
      "      1 ablution\n",
      "      1 abound\n",
      "      1 abrupt\n",
      "      1 absurdity\n",
      "      1 abundant\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr '[A-Z]' '[a-z]' < data/pride.txt |\n",
    "tr -sc '[a-z]' '[\\012*]' |\n",
    "sort |\n",
    "uniq -c |\n",
    "sort -n | sed 10q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-guidance",
   "metadata": {},
   "source": [
    "To sum up: Pipelines are a honking good idea and together with the input '<' and output '>' operators we can move forwards quickly. We can use 'tr' to tokenize in different ways, according to the context. And we have different ways of sorting at our disposal:\n",
    "\n",
    "| Example | Explanation |\n",
    "| :- | :- |\n",
    "| sort -d | dictionary order |\n",
    "| sort -f | fold case |\n",
    "| sort -n | numeric order |\n",
    "| sort -nr | reverse numeric order |\n",
    "| sort -u | remove duplicates |\n",
    "| sort +1 | start with field 1 (start with 0) |\n",
    "| sort +0.50 | start with 50th char (of first field == 0) |\n",
    "| sort +1.5 | start with 5th char of field 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-grill",
   "metadata": {},
   "source": [
    "#### Bigrams\n",
    "\n",
    "Bigrams are pair of words that can present us with a somewhat better, although still limited, view of the semantics of a text because we pair 2 adjacent words, instead of a simple bag of words (bow).\n",
    "\n",
    "We start with a part of the code we used above to generate our list of words, all lowercase and we write the result to a file: pride.words.\n",
    "\n",
    "Then the new part of our code snippet: We use that file to generate a new file with all words BUT rotate them a place and output the result to a second file: pride.nextwords\n",
    "\n",
    "We can use both files to paste the lines together and save the result into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "central-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tr '[A-Z]' '[a-z]' < data/pride.txt |\n",
    "tr -sc '[a-z]' '[\\012*]' > data/pride.words\n",
    "tail +2 data/pride.words > data/pride.nextwords\n",
    "paste data/pride.words data/pride.nextwords |\n",
    "sort | uniq -c > data/pride.bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-prefix",
   "metadata": {},
   "source": [
    "Have a look at the most used bigrams with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "political-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    464 of\tthe\n",
      "    443 to\tbe\n",
      "    382 in\tthe\n",
      "    303 i\tam\n",
      "    273 mr\tdarcy\n",
      "    261 of\ther\n",
      "    252 to\tthe\n",
      "    251 it\twas\n",
      "    235 of\this\n",
      "    212 she\twas\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sort -nr < data/pride.bigrams | sed 10q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-coach",
   "metadata": {},
   "source": [
    "#### Grep and friends\n",
    "\n",
    "Another way of quickly scanning texts and selecting lines containin some words is to use the grep utility (or one of its friends: egrep, ack, etc.).\n",
    "\n",
    "Suppose we are interested in lines that contain a reference to Mr. Darcy, we can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coastal-writing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looked the gentleman; but his friend Mr. Darcy soon drew the attention\r\n",
      "themselves. What a contrast between him and his friend! Mr. Darcy danced\r\n",
      "down for two dances; and during part of that time, Mr. Darcy had been\r\n",
      "“Come, Darcy,” said he, “I must have you dance. I hate to see you\r\n",
      "Darcy, looking at the eldest Miss Bennet.\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep 'Darcy' data/pride.txt | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-energy",
   "metadata": {},
   "source": [
    "But we can also use our new bigrams file to grep bigrams with Darcy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indoor-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 a\tdarcy\n",
      "      1 added\tdarcy\n",
      "      1 affairs\tdarcy\n",
      "      1 all\tdarcy\n",
      "      8 and\tdarcy\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep 'darcy' data/pride.bigrams | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-estate",
   "metadata": {},
   "source": [
    "In the two bigram examples above we see that one has to be very interested in the use of words like \"of\", \"to\", \"in\" by Jane Austen in order to appreciate the information generated. Depending on the questions we have, we often want to discard certain parts, words of a text, in oder to be able to concentrate on the parts we are interested in.\n",
    "\n",
    "Let's use our friend grep to filter out some of the stopwords.\n",
    "\n",
    "For that we make a file \"stopwords\" that contains the words, each on a line of its own, that we are NOT interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unique-banks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200 had\tbeen\n",
      "    167 could\tnot\n",
      "    166 he\thad\n",
      "    153 mrs\tbennet\n",
      "    135 have\tbeen\n",
      "    133 they\twere\n",
      "    132 did\tnot\n",
      "    124 do\tnot\n",
      "    120 my\tdear\n",
      "    116 lady\tcatherine\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr '[A-Z]' '[a-z]' < data/pride.txt |\n",
    "tr -sc '[a-z]' '[\\012*]' > data/pride.words\n",
    "tail +2 data/pride.words > data/pride.nextwords\n",
    "paste data/pride.words data/pride.nextwords |\n",
    "grep -v -w -f data/stopwords |\n",
    "sort | uniq -c > data/pride.bigrams\n",
    "sort -nr < data/pride.bigrams | sed 10q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-martin",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Depending on the files you start to work with, you should be prepared to do some cleaning up. Depending on the sources you have and the questions you have, you further tailor the sources to work with.\n",
    "\n",
    "The small programs we used above are so-called Unix utilities. You will have to work on a Unix, Linux or MacOS machine to be able to use them. And they much more powerful then we have showed here.\n",
    "\n",
    "Here they were just used to give you an idea of what it is to work with texts. You have to be able to see what you have got to work with, for example by using a plain text editor.\n",
    "\n",
    "Even with the small examples above our data subdirectory now contains 6 files. We started with the text file of Pride and Prejudice and generated 5 files. Somehow we must keep track of the intermediates and results we generate: Version control software.\n",
    "\n",
    "What we did and why we did what we did is stated in this file. A so-called Jupyter notebook in which we can use text cells together with cells that contain code and generate the results of running that code.\n",
    "\n",
    "We could have done a lot more with the Jane Austen book. Analyse the sentences in which the male characters are mentioned against the ones in which the female characters play a role. Zoom in on the sentences that contain dialogue. Or we could have "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
