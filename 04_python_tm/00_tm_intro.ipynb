{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "restricted-holocaust",
   "metadata": {},
   "source": [
    "### Text mining intro\n",
    "\n",
    "This Jupyter notebook is meant to give you a feel for working with files in the context of textual analysis:\n",
    "\n",
    "    - I have downloaded these files from WoS but what is actually there? (editor);\n",
    "    - Right, actually I am just interested in a small portion of what I got (text extraction);\n",
    "    - Ok, this is not what I had in mind (go back to the source) or: prepare to do some serious pre-processing.\n",
    "\n",
    "Farfetched examples? Not at all.\n",
    "\n",
    "The examples are taken from Kenneth Ward Church: Unix(tm) for Poets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-leisure",
   "metadata": {},
   "source": [
    "### Jane Austen: Pride and prejudice\n",
    "\n",
    "The Manning book \"Natural Language Processing in Action\" (NLPIA) comes with some example files. These example files are, more often than not, prepared files. In other words, the files are already customized for the software the authors use (for example: Every sentence is on one line (terminated by a newline+) instead of have formatted text that is not wider than 79 characters, each physical line (possibly part of a sentence) ended by a newline+.\n",
    "\n",
    "The NLPIA file is: pride.txt\n",
    "\n",
    "If we open this file in our editor of choice, we \"see\" the formatting. Each sentence in the book is a paragraph, that is there are 2 newlines between sentences (here: sequences of characters terminated by a full stop). There is ONE newline that chops up a book sentence into more or less equal parts to display on the screen.\n",
    "\n",
    "This might not seem a big deal at first glance, but there are all sort of possible glitches right from the start. How many book sentences does P&P have? We can count the double newlines. In my editor that gives 1866 book sentences.\n",
    "\n",
    "Suppose our software can only deal with book sentences on one line in our file, what to do? Easy peasy: Just replace single newlines by a space and double newlines by a single newline. *What could go possibly wrong? Can you spot the caveat?*\n",
    "\n",
    "In the real world out there things are different. If we download the Jane Austen ook from Project Gutenberg, we can choose the format. In our case we choose the plain text UTF-8 file.\n",
    "\n",
    "http://www.gutenberg.org/ebooks/1342\n",
    "\n",
    "If we open this file in our editor, we see something different. The beginning of the first book sentence \"It is a truth universally acknowledged ...\" starts on line 167 in our file. Then the text becomes familiar until the very end where the sequence *** END OF ... signals text added by Project Gutenberg.\n",
    "\n",
    "The NLPIA authors simply removed the Project Gutenberg sandwich around the Jane Austen text, which makes perfect sense if one is going to do textual analysis of that text.\n",
    "\n",
    "#### Conclusions\n",
    "\n",
    "1. It pays off to know the basics of a so-called programming editor:\n",
    "\n",
    "    - Inspect what is there;\n",
    "    - Make some quick necessary changes (re-formatting, deleting the Gutenberg sandwich, etc.) When mistakes are made these editors allow for undo;\n",
    "    - When the file is saved, we save plain text in UTF-8.\n",
    "\n",
    "2. If we were to start with the Project Gutenberg file, we end up with 2 files right at the beginning of our project. We can store the PG file we got from their website as a source file (for example in a directory named \"src\" or \"raw\" and the pre-processed file in another dierctory \"wip\".\n",
    "\n",
    "3. If one really dives in files in different stages tend to pile up. If that is the case simple version control routines (we use Git) can be of tremendous help. If you reached a certain result, you check that particular result in -- as a snapshot. And just as undo comes in handy, these snapshots can be used to roll back to after you messed up things big time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-weight",
   "metadata": {},
   "source": [
    "Now that we have some idea of the environment we need to do our textmining work, we can explore some basic tasks without diving to deep into prgramming (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-hierarchy",
   "metadata": {},
   "source": [
    "### The Command Line Interface (CLI)\n",
    "\n",
    "The CLI or terminal a bit Unix/Linux and MacOS oriented; MS Windows users must use a workaround. For the remaining part of this workshop we will concentrate on working with Python programs and code via Jupyter notebooks. But having a glimpse of some of the CLI possibilities not only shows how powerful these tools are, but, more importantly, can show some important concepts of working with textual data:\n",
    "\n",
    "    - chunking running plain text into meaningful chunks;\n",
    "    - discarding parts of the text;\n",
    "    - pipelines, the combination of small code snippets that, in a series of steps, produce the desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-wedding",
   "metadata": {},
   "source": [
    "#### Let's have a quick look at our text\n",
    "\n",
    "First 5 sentences of P&P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governmental-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿It is a truth universally acknowledged, that a single man in possession\r\n",
      "of a good fortune, must be in want of a wife.\r\n",
      "\r\n",
      "However little known the feelings or views of such a man may be on his\r\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sed 5q < data/pride.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-thanks",
   "metadata": {},
   "source": [
    "#### Count words in a text\n",
    "\n",
    "We break up this task in three discrete steps:\n",
    "\n",
    "1. We break up the text into a sequence of words (or tokens => tokenizing) with tr\n",
    "2. We then sort all the words in our sequence with sort\n",
    "3. Then we count the duplicates with uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-isolation",
   "metadata": {},
   "source": [
    "Let's try to break up our text in tokens (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "treated-industry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It\n",
      "is\n",
      "a\n",
      "truth\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-brush",
   "metadata": {},
   "source": [
    "Fine, works like a charm. And, what is more important, with tr we manipulated newlines like a breeze. Usually newlines are a pain in the proverbial. They can mean the end of a line or a command (depends on the context; but remember in my editor I had to enter the magic combo 'CTRL-Q CTRL-J' twice to select all \"\\n\\n\" sequneces. Here tr let us use the octal representation of the newline char: '\\012*'. Perfect.\n",
    "\n",
    "Let's sort our sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exact-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt |\n",
    "sort | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-railway",
   "metadata": {},
   "source": [
    "Next we count the number of occurences of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exempt-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "   1905 a\n",
      "     49 A\n",
      "      1 abatement\n",
      "      6 abhorrence\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt |\n",
    "sort |\n",
    "uniq -c | sed 5q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sized-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4113 to\n",
      "   4058 the\n",
      "   3599 of\n",
      "   3430 and\n",
      "   2138 her\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr -sc '[A-Z][a-z]' '[\\012*]' < data/pride.txt |\n",
    "sort |\n",
    "uniq -c |\n",
    "sort -nr | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-mirror",
   "metadata": {},
   "source": [
    "The important thing in the above code snippets are not the somewhat cryptic and terse commands, but the ease of gluing these commands together in a so-called \"pipeline\" using the Unix pipe symbol \"|\" where the output of one command becomes the input of the next. This allows for fast experimenting and adjusting our snippets.\n",
    "\n",
    "The output of the snippet in cell [4] shows that we count 1905 occurences for the lowercase 'a' and 49 for the uppercase 'A'. With our friend tr these are easily merged, we just cast all tokens to lowercase: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thirty-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "   1954 a\n",
      "      1 abatement\n",
      "      6 abhorrence\n",
      "      1 abhorrent\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr '[A-Z]' '[a-z]' < data/pride.txt |\n",
    "tr -sc '[a-z]' '[\\012*]' |\n",
    "sort |\n",
    "uniq -c | sed 5q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-public",
   "metadata": {},
   "source": [
    "Right we use the new snippet for our counting pipeline, but this time we do not reverse the order, so we get the least used words first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "female-prisoner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "      1 abatement\n",
      "      1 abhorrent\n",
      "      1 abide\n",
      "      1 abiding\n",
      "      1 ablution\n",
      "      1 abound\n",
      "      1 abrupt\n",
      "      1 absurdity\n",
      "      1 abundant\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tr '[A-Z]' '[a-z]' < data/pride.txt |\n",
    "tr -sc '[a-z]' '[\\012*]' |\n",
    "sort |\n",
    "uniq -c |\n",
    "sort -n | sed 10q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-racing",
   "metadata": {},
   "source": [
    "To sum up: Pipelines are a honking good idea and together with the input '<' and output '>' operators we can move forwards quickly. We can use 'tr' to tokenize in different ways, according to the context. And we have different ways of sorting at our disposal:\n",
    "\n",
    "| Example | Explanation |\n",
    "| :- | :- |\n",
    "| sort -d | dictionary order |\n",
    "| sort -f | fold case |\n",
    "| sort -n | numeric order |\n",
    "| sort -nr | reverse numeric order |\n",
    "| sort -u | remove duplicates |\n",
    "| sort +1 | start with field 1 (start with 0) |\n",
    "| sort +0.50 | start with 50th char (of first field == 0) |\n",
    "| sort +1.5 | start with 5th char of field 1 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
